{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in b:\\downloads\\anaconda\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in b:\\downloads\\anaconda\\lib\\site-packages (from imblearn) (0.3.3)\n",
      "Requirement already satisfied: scipy in b:\\downloads\\anaconda\\lib\\site-packages (from imbalanced-learn->imblearn) (1.1.0)\n",
      "Requirement already satisfied: numpy in b:\\downloads\\anaconda\\lib\\site-packages (from imbalanced-learn->imblearn) (1.14.3)\n",
      "Requirement already satisfied: scikit-learn in b:\\downloads\\anaconda\\lib\\site-packages (from imbalanced-learn->imblearn) (0.19.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "You are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# To supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Basic Libraries for Data organization, Statistical operations and Plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "# For loading .arff files\n",
    "from scipy.io import arff\n",
    "# To analyze the type of missing data\n",
    "#!pip install missingno\n",
    "#import missingno as msno\n",
    "# Library for performing k-NN and MICE imputations \n",
    "#!pip install fancyimpute\n",
    "#import fancyimpute\n",
    "# Library to perform Expectation-Maximization (EM) imputation\n",
    "#!pip install impyute\n",
    "#import impyute as impy\n",
    "# To perform mean imputation\n",
    "from sklearn.preprocessing import Imputer\n",
    "#To perform kFold Cross Validation\n",
    "from sklearn.model_selection import KFold\n",
    "# Formatted counter of class labels\n",
    "from collections import Counter\n",
    "# Ordered Dictionary\n",
    "from collections import OrderedDict\n",
    "# Library imbalanced-learn to deal with the data imbalance. To use SMOTE oversampling\n",
    "!pip install imblearn\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "# Impoting classification models\n",
    "#!pip install xgboost\n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Loads the 5 raw .arff files into a list\n",
    "def load_arff_raw_data():\n",
    "    N=5\n",
    "    return [arff.loadarff('data/' + str(i+1) + 'year.arff') for i in range(N)]\n",
    "\n",
    "############################################################\n",
    "# Loads the 5 raw .arff files into pandas dataframes\n",
    "def load_dataframes():\n",
    "    return [pd.DataFrame(data_i_year[0]) for data_i_year in load_arff_raw_data()]\n",
    "\n",
    "############################################################\n",
    "# Set the column headers from X1 ... X64 and the class label as Y, for all the 5 dataframes.\n",
    "def set_new_headers(dataframes):\n",
    "    cols = ['X' + str(i+1) for i in range(len(dataframes[0].columns)-1)]\n",
    "    cols.append('Y')\n",
    "    for df in dataframes:\n",
    "        df.columns = cols\n",
    "\n",
    "############################################################\n",
    "# dataframes is the list of pandas dataframes for the 5 year datafiles.  \n",
    "dataframes = load_dataframes()\n",
    "\n",
    "# Set the new headers for the dataframes. The new headers will have the renamed set of feature (X1 to X64)\n",
    "set_new_headers(dataframes)    \n",
    "\n",
    "# print the first 5 rows of a dataset 'year1'\n",
    "dataframes[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dtypes of all the columns (other than the class label columns) to float.\n",
    "def convert_columns_type_float(dfs):\n",
    "    for i in range(5):\n",
    "        index = 1\n",
    "        while(index<=63):\n",
    "            colname = dfs[i].columns[index]\n",
    "            col = getattr(dfs[i], colname)\n",
    "            dfs[i][colname] = col.astype(float)\n",
    "            index+=1\n",
    "            \n",
    "convert_columns_type_float(dataframes)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class labels for all the dataframes are originally in object type.\n",
    "# Convert them to int types\n",
    "def convert_class_label_type_int(dfs):\n",
    "    for i in range(len(dfs)):\n",
    "        col = getattr(dfs[i], 'Y')\n",
    "        dfs[i]['Y'] = col.astype(int)\n",
    "        \n",
    "convert_class_label_type_int(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Get Clean dataframes by dropping all the rows which have missing values\n",
    "def drop_nan_rows(dataframes, verbose=False):\n",
    "    clean_dataframes = [df.dropna(axis=0, how='any') for df in dataframes]\n",
    "    if verbose:\n",
    "        for i in range(len(dataframes)):\n",
    "            print(str(i+1)+'year:','Original Length=', len(dataframes[i]), '\\tCleaned Length=', len(clean_dataframes[i]), '\\tMissing Data=', len(dataframes[i])-len(clean_dataframes[i]))\n",
    "    return clean_dataframes\n",
    "\n",
    "# Doing a quick analysis of how many missing values are there in each of the 5 dataframes\n",
    "nan_dropped_dataframes = drop_nan_rows(dataframes, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_mean_imputation(dfs):\n",
    "    # Construct an imputer with strategy as 'mean', to mean-impute along the columns\n",
    "    imputer = Imputer(missing_values=np.nan, strategy='mean', axis=0)\n",
    "    mean_imputed_dfs = [pd.DataFrame(imputer.fit_transform(df)) for df in dfs]\n",
    "    for i in range(len(dfs)):\n",
    "        mean_imputed_dfs[i].columns = dfs[i].columns   \n",
    "    return mean_imputed_dfs\n",
    "\n",
    "mean_imputed_dataframes = perform_mean_imputation(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_dataframes_dictionary = OrderedDict()\n",
    "imputed_dataframes_dictionary['Mean'] = mean_imputed_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_imbalance(dfs):\n",
    "    for i in range(len(dfs)):\n",
    "        print('Dataset: '+str(i+1)+'year')\n",
    "        print(dfs[i].groupby('Y').size())\n",
    "        minority_percent = (dfs[i]['Y'].tolist().count(1) / len(dfs[i]['Y'].tolist()))*100\n",
    "        print('Minority (label 1) percentage: '+  str(minority_percent) + '%')\n",
    "        print('-'*64)\n",
    "        \n",
    "check_data_imbalance(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the features and labels into separate dataframes for all the original dataframes\n",
    "def split_dataframes_features_labels(dfs):\n",
    "    feature_dfs = [dfs[i].iloc[:,0:64] for i in range(len(dfs))]\n",
    "    label_dfs = [dfs[i].iloc[:,64] for i in range(len(dfs))]\n",
    "    return feature_dfs, label_dfs\n",
    "\n",
    "# Performs the SMOTE oversampling fro given dataframes.\n",
    "def oversample_data_SMOTE(dfs, verbose=False):\n",
    "    smote = SMOTE(ratio='auto' , random_state=42, k_neighbors=10)\n",
    "    #Split the features and labels for each dataframe\n",
    "    feature_dfs, label_dfs = split_dataframes_features_labels(dfs)\n",
    "    resampled_feature_arrays = []\n",
    "    resampled_label_arrays = []\n",
    "    for i in range(len(dfs)):\n",
    "        if verbose: print('Dataset: ' + str(i+1) + 'year:')\n",
    "        if verbose: print('Original dataset shape {}'.format(Counter(label_dfs[i])))\n",
    "        dfi_features_res, dfi_label_res = smote.fit_sample(feature_dfs[i], label_dfs[i])\n",
    "        if verbose: print('Resampled dataset shape {}\\n'.format(Counter(dfi_label_res)))\n",
    "        # Append the resampled feature and label arrays of ith dataframe to their respective list of arrays    \n",
    "        resampled_feature_arrays.append(dfi_features_res)\n",
    "        resampled_label_arrays.append(dfi_label_res)        \n",
    "    return resampled_feature_arrays, resampled_label_arrays\n",
    "\n",
    "# Utility Function to convert the arrays of features and labels to pandas dataframes, and then join them.\n",
    "# Also re-assign the columns headers.\n",
    "def restructure_arrays_to_dataframes(feature_arrays, label_arrays):\n",
    "    resampled_dfs = []\n",
    "    for i in range(len(feature_arrays)):\n",
    "        feature_df = pd.DataFrame(data=feature_arrays[i])\n",
    "        label_df = pd.DataFrame(data=label_arrays[i])\n",
    "        # Must set the column header for label_df, otherwise it wont join with feature_df, as columns overlap (with col names '0')\n",
    "        label_df.columns=['Y'] \n",
    "        resampled_dfs.append(feature_df.join(label_df))\n",
    "    # re-assign the column headers for features and labels    \n",
    "    set_new_headers(resampled_dfs)    \n",
    "    return resampled_dfs\n",
    "\n",
    "# Perform SMOTE oversampling on all the imputed dataframes, and return them in a dictionary.\n",
    "def perform_oversampling_on_imputed_dataframes(df_dict):\n",
    "    imputed_oversampled_dataframes_dictionary = OrderedDict()\n",
    "    for key,dfs in df_dict.items():\n",
    "        print('SMOTE Oversampling for ' + key + ' imputed dataframes\\n')\n",
    "        smote_feature_arrays, smote_label_arrays = oversample_data_SMOTE(dfs, verbose=True)\n",
    "        oversampled_dataframes = restructure_arrays_to_dataframes(smote_feature_arrays, smote_label_arrays)\n",
    "        imputed_oversampled_dataframes_dictionary[key] = oversampled_dataframes\n",
    "        print('-'*100)\n",
    "    return imputed_oversampled_dataframes_dictionary\n",
    "\n",
    "imputed_oversampled_dataframes_dictionary = perform_oversampling_on_imputed_dataframes(imputed_dataframes_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_kfold_cv_data(k, X, y, verbose=False):\n",
    "    X = X.values\n",
    "    y = y.values\n",
    "    kf = KFold(n_splits=k, shuffle=False, random_state=42)\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train.append(X[train_index])\n",
    "        y_train.append(y[train_index])\n",
    "        X_test.append(X[test_index])\n",
    "        y_test.append(y[test_index])\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes classifier\n",
    "gnb_classifier = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression classifier\n",
    "lr_classifier = LogisticRegression(penalty = 'l1', random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators = 5, criterion = 'entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced Bagging Classifier\n",
    "bb_classifier = BalancedBaggingClassifier(base_estimator = RandomForestClassifier(criterion='entropy'), n_estimators = 5, bootstrap = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary of models\n",
    "models_dictionary = OrderedDict()\n",
    "\n",
    "models_dictionary['Gaussian Naive Bayes'] = gnb_classifier\n",
    "models_dictionary['Logistic Regression'] = lr_classifier\n",
    "models_dictionary['Decision Tree'] = dt_classifier\n",
    "models_dictionary['Random Forest'] = rf_classifier\n",
    "models_dictionary['Balanced Bagging'] = bb_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform data modeling\n",
    "def perform_data_modeling(_models_, _imputers_, verbose=False, k_folds=5):\n",
    "    \n",
    "    # 7 Models\n",
    "    # 4 Imputers\n",
    "    # 5 datasets (for 5 years)\n",
    "    # 7 metrics, averaged over all the K-Folds\n",
    "    model_results = OrderedDict()\n",
    "    \n",
    "    # Iterate over the models\n",
    "    for model_name, clf in _models_.items():\n",
    "        if verbose: print(\"-\"*120, \"\\n\", \"Model: \" + '\\033[1m' + model_name + '\\033[0m' + \" Classifier\")\n",
    "        imputer_results = OrderedDict()\n",
    "        \n",
    "        # Iterate over the different imputed_data mechanisms (Mean, k-NN, EM, MICE)\n",
    "        for imputer_name, dataframes_list in _imputers_.items():\n",
    "            if verbose: print('\\tImputer Technique: ' + '\\033[1m' + imputer_name + '\\033[0m')\n",
    "            \n",
    "            # call the split_dataframes_features_labels function to get a list of features and labels for all the dataframes\n",
    "            feature_dfs, label_dfs = split_dataframes_features_labels(dataframes_list)            \n",
    "            \n",
    "            year_results = OrderedDict()\n",
    "            \n",
    "            # Iterate over dataframe_list individually\n",
    "            for df_index in range(len(dataframes_list)):\n",
    "                if verbose: print('\\t\\tDataset: ' + '\\033[1m' + str(df_index+1) + 'year' + '\\033[0m')\n",
    "                \n",
    "                # Calling the 'prepare_kfold_cv_data' returns lists of features and labels \n",
    "                # for train and test sets respectively.\n",
    "                # The number of items in the list is equal to k_folds\n",
    "                X_train_list, y_train_list, X_test_list, y_test_list = prepare_kfold_cv_data(k_folds, feature_dfs[df_index], label_dfs[df_index], verbose)\n",
    "                \n",
    "                metrics_results = OrderedDict()\n",
    "                accuracy_list = np.zeros([k_folds])\n",
    "                precision_list = np.zeros([k_folds,2])\n",
    "                recall_list = np.zeros([k_folds,2])\n",
    "                TN_list = np.zeros([k_folds])\n",
    "                FP_list = np.zeros([k_folds])\n",
    "                FN_list = np.zeros([k_folds])\n",
    "                TP_list = np.zeros([k_folds])                \n",
    "                \n",
    "                # Iterate over all the k-folds\n",
    "                for k_index in range(k_folds):\n",
    "                    X_train = X_train_list[k_index]\n",
    "                    y_train = y_train_list[k_index]\n",
    "                    X_test = X_test_list[k_index]\n",
    "                    y_test = y_test_list[k_index]\n",
    "                    \n",
    "                    # Fit the model and \n",
    "                    clf = clf.fit(X_train, y_train)\n",
    "                    y_test_predicted = clf.predict(X_test)\n",
    "                    \n",
    "                    #code for calculating accuracy \n",
    "                    _accuracy_ = accuracy_score(y_test, y_test_predicted, normalize=True)\n",
    "                    accuracy_list[k_index] = _accuracy_\n",
    "                    \n",
    "                    #code for calculating recall \n",
    "                    _recalls_ = recall_score(y_test, y_test_predicted, average=None)\n",
    "                    recall_list[k_index] = _recalls_\n",
    "                    \n",
    "                    #code for calculating precision \n",
    "                    _precisions_ = precision_score(y_test, y_test_predicted, average=None)\n",
    "                    precision_list[k_index] = _precisions_\n",
    "                    \n",
    "                    #code for calculating confusion matrix \n",
    "                    _confusion_matrix_ = confusion_matrix(y_test, y_test_predicted)\n",
    "                    TN_list[k_index] = _confusion_matrix_[0][0]\n",
    "                    FP_list[k_index] = _confusion_matrix_[0][1]\n",
    "                    FN_list[k_index] = _confusion_matrix_[1][0]\n",
    "                    TP_list[k_index] = _confusion_matrix_[1][1]\n",
    "                \n",
    "                # creating a metrics dictionary\n",
    "                metrics_results['Accuracy'] = np.mean(accuracy_list)\n",
    "                metrics_results['Precisions'] = np.mean(precision_list, axis=0)\n",
    "                metrics_results['Recalls'] = np.mean(recall_list, axis=0)\n",
    "                metrics_results['TN'] = np.mean(TN_list)\n",
    "                metrics_results['FP'] = np.mean(FP_list)\n",
    "                metrics_results['FN'] = np.mean(FN_list)\n",
    "                metrics_results['TP'] = np.mean(TP_list)\n",
    "                \n",
    "                if verbose:\n",
    "                    print('\\t\\t\\tAccuracy:', metrics_results['Accuracy'])\n",
    "                    print('\\t\\t\\tPrecision:', metrics_results['Precisions'])\n",
    "                    print('\\t\\t\\tRecall:', metrics_results['Recalls'])\n",
    "                \n",
    "                year_results[str(df_index+1)+'year'] = metrics_results   \n",
    "                \n",
    "            imputer_results[imputer_name] = year_results\n",
    "            \n",
    "        model_results[model_name] = imputer_results  \n",
    "        \n",
    "    return model_results                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = perform_data_modeling(models_dictionary, imputed_oversampled_dataframes_dictionary, verbose=True, k_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model -> imputer -> year\n",
    "def perform_model_ranking(models, imputers, results):\n",
    "    column_headers = ['-'] + list(imputers.keys())\n",
    "    rows = []\n",
    "    for model_name, model_details in results.items():\n",
    "        row = [model_name]\n",
    "        for imputer_name, imputer_details in model_details.items():\n",
    "            mean_accuracy = 0\n",
    "            for year, metrics in imputer_details.items():\n",
    "                mean_accuracy += metrics['Accuracy']\n",
    "            mean_accuracy = mean_accuracy/len(imputer_details)\n",
    "            row.append(mean_accuracy)\n",
    "        rows.append(row)\n",
    "    results_df = pd.DataFrame(data=rows, columns = column_headers)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_model_ranking(models_dictionary, imputed_oversampled_dataframes_dictionary, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This list stores results of Balanced Bagging classifier obtained by running it for \n",
    "# various values of number of estimators in the range of 1 to 30\n",
    "results_by_estimators = []\n",
    "for i in range(29):\n",
    "    models_dictionary['Balanced Bagging'] = BalancedBaggingClassifier(base_estimator = RandomForestClassifier(criterion='entropy'), n_estimators = 1+i, bootstrap = True)\n",
    "    results = perform_data_modeling(models_dictionary, imputed_oversampled_dataframes_dictionary, verbose=True, k_folds=5)\n",
    "    results_by_estimators.append(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year1_values = []\n",
    "year2_values = []\n",
    "year3_values = []\n",
    "year4_values = []\n",
    "year5_values = []\n",
    "\n",
    "# extract corresponding Balanced bagging with Mean imputation\n",
    "# classification metrics \n",
    "def extract_actual_values_from_dict(curr_dict):\n",
    "    temp_dict = curr_dict['Balanced Bagging']\n",
    "    return temp_dict['Mean']\n",
    "\n",
    "for i in range(29):\n",
    "    curr_dict = results_by_estimators[i]\n",
    "    curr_result = extract_actual_values_from_dict(curr_dict)\n",
    "    \n",
    "        \n",
    "    year_1_result = curr_result['1year']\n",
    "    year_2_result = curr_result['2year']\n",
    "    year_3_result = curr_result['3year']\n",
    "    year_4_result = curr_result['4year']\n",
    "    year_5_result = curr_result['5year']\n",
    "    year1_values.append(year_1_result['Accuracy'])\n",
    "    year2_values.append(year_2_result['Accuracy'])\n",
    "    year3_values.append(year_3_result['Accuracy'])\n",
    "    year4_values.append(year_4_result['Accuracy'])\n",
    "    year5_values.append(year_5_result['Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "estimators = [i+1 for i in range(29)] \n",
    "\n",
    "# plot year1, year2, year3, year4 and year5 accuracy values\n",
    "# for range of estimator values from 1 to 30\n",
    "plt.plot(estimators, year1_values, '.b-')\n",
    "plt.plot(estimators, year2_values, '.r-')\n",
    "plt.plot(estimators, year3_values, '.y-')\n",
    "plt.plot(estimators, year4_values, '.g-')\n",
    "plt.plot(estimators, year5_values, '.m-') \n",
    "plt.xlabel(\"\\nNumber of estimators\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"\\nEffect of varying number of estimators on the accuracy scores on different datasets\\n\")\n",
    "\n",
    "# display legend\n",
    "plt.plot(10, 0.93, '.b-', label='Year 1')\n",
    "plt.plot(10, 0.93, '.r-', label='Year 2')\n",
    "plt.plot(10, 0.93, '.y-', label='Year 3')\n",
    "plt.plot(10, 0.93, '.g-', label='Year 4')\n",
    "plt.plot(10, 0.93, '.m-', label='Year 5')\n",
    "\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
