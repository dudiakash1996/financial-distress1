{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_read(file,file1):\n",
    "    # To supress warnings\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    #!pip install msgpack-python\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    #!pip install pickle  --upgrade pip\n",
    "    import pickle\n",
    "    %matplotlib inline\n",
    "\n",
    "    # For loading .arff files\n",
    "    from scipy.io import arff\n",
    "\n",
    "    # To perform mean imputation\n",
    "    from sklearn.preprocessing import Imputer\n",
    "\n",
    "    # Formatted counter of class labels\n",
    "    from collections import Counter\n",
    "    # Ordered Dictionary\n",
    "    from collections import OrderedDict\n",
    "    #To perform kFold Cross Validation\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Library imbalanced-learn to deal with the data imbalance. To use SMOTE oversampling\n",
    "    !pip install imblearn\n",
    "    from imblearn.over_sampling import SMOTE \n",
    "\n",
    "    # Impoting classification models\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from imblearn.ensemble import BalancedBaggingClassifier\n",
    "\n",
    "    import random\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    \n",
    "    \n",
    "    df1, meta1 = arff.loadarff(file);\n",
    "    df1 = pd.DataFrame(df1);\n",
    "    df2, meta2 = arff.loadarff(file1);\n",
    "    df2 = pd.DataFrame(df2);\n",
    "    \n",
    "    \n",
    "    def set_new_headers(dataframes):\n",
    "        cols = ['X' + str(i+1) for i in range(len(dataframes.columns)-1)]\n",
    "        cols.append('Y')\n",
    "        dataframes.columns = cols\n",
    "    set_new_headers(df1)\n",
    "    set_new_headers(df2)\n",
    "    \n",
    "    \n",
    "    # Convert the dtypes of all the columns (other than the class label columns) to float.\n",
    "    def convert_columns_type_float(dfs):\n",
    "        index = 1\n",
    "        while(index<=63):\n",
    "            colname = dfs.columns[index]\n",
    "            col = getattr(dfs, colname)\n",
    "            dfs[colname] = col.astype(float)\n",
    "            index+=1\n",
    "\n",
    "    convert_columns_type_float(df1)\n",
    "    convert_columns_type_float(df2)\n",
    "\n",
    "\n",
    "    # The class labels for all the dataframes are originally in object type.\n",
    "    # Convert them to int types\n",
    "    def convert_class_label_type_int(dfs):\n",
    "        col = getattr(dfs, 'Y')\n",
    "        dfs['Y'] = col.astype(int)\n",
    "\n",
    "        \n",
    "    convert_class_label_type_int(df1)\n",
    "    convert_class_label_type_int(df2)\n",
    "    \n",
    "    # Get Clean dataframes by dropping all the rows which have missing values\n",
    "    def drop_nan_rows(dataframes, verbose=False):\n",
    "        clean_dataframes = dataframes.dropna(axis=0, how='any')\n",
    "        if verbose:\n",
    "            print('Original Length=', len(dataframes), '\\tCleaned Length=', len(clean_dataframes), '\\tMissing Data=', len(dataframes)-len(clean_dataframes))\n",
    "        return clean_dataframes\n",
    "\n",
    "    # Doing a quick analysis of how many missing values are there in each of the 5 dataframes\n",
    "    nan_dropped_dataframes = drop_nan_rows(df1, verbose=True)\n",
    "    nan_dropped_dataframes1 = drop_nan_rows(df2, verbose=True)\n",
    "    \n",
    "    def perform_mean_imputation(dfs):\n",
    "        # Construct an imputer with strategy as 'mean', to mean-impute along the columns\n",
    "        imputer = Imputer(missing_values=np.nan, strategy='mean', axis=0)\n",
    "        mean_imputed_dfs = pd.DataFrame(imputer.fit_transform(dfs))\n",
    "        mean_imputed_dfs.columns = dfs.columns   \n",
    "        return mean_imputed_dfs\n",
    "\n",
    "    mean_imputed_dataframes = perform_mean_imputation(df1)\n",
    "    mean_imputed_dataframes1 = perform_mean_imputation(df2)\n",
    "    \n",
    "    \n",
    "    def check_data_imbalance(dfs):\n",
    "        print('Dataset: '+str(1)+'styear')\n",
    "        print(dfs.groupby('Y').size())\n",
    "        minority_percent = (dfs['Y'].tolist().count(1) / len(dfs['Y'].tolist()))*100\n",
    "        print('Minority (label 1) percentage: '+  str(minority_percent) + '%')\n",
    "        print('-'*64)\n",
    "\n",
    "    check_data_imbalance(df1)\n",
    "    check_data_imbalance(df2)\n",
    "    \n",
    "    # Split the features and labels into separate dataframes for all the original dataframes\n",
    "    def split_dataframes_features_labels(dfs):\n",
    "        feature_dfs = dfs.iloc[:,0:64] \n",
    "        label_dfs = dfs.iloc[:,64]\n",
    "        return feature_dfs, label_dfs\n",
    "    feature,label = split_dataframes_features_labels(mean_imputed_dataframes)\n",
    "    feature1,label1 = split_dataframes_features_labels(mean_imputed_dataframes1)\n",
    "    m1 = SMOTE()\n",
    "    x,y = m1.fit_sample(feature,label)\n",
    "    x = pd.DataFrame(x)\n",
    "    y = pd.DataFrame(y)\n",
    "    data = pd.concat([x,y],axis = 1)\n",
    "    set_new_headers(data)\n",
    "    data_feature = data.iloc[:,0:64] \n",
    "    data_label = data.iloc[:,64:]\n",
    "    data_feature_train,data_feature_test,data_label_train,data_label_test = train_test_split(data_feature,data_label,test_size = .3,random_state = 0)\n",
    "\n",
    "    x1,y1 = m1.fit_sample(feature1,label1)\n",
    "    x1 = pd.DataFrame(x1)\n",
    "    y1 = pd.DataFrame(y1)\n",
    "    data1 = pd.concat([x1,y1],axis = 1)\n",
    "    set_new_headers(data1)\n",
    "    data_feature1 = data1.iloc[:,0:64] \n",
    "    data_label1 = data1.iloc[:,64:]\n",
    "    data_feature_train1,data_feature_test1,data_label_train1,data_label_test1 = train_test_split(data_feature1,data_label1,test_size = .3,random_state = 0)\n",
    "\n",
    "    # Balanced Bagging Classifier\n",
    "    bb_classifier = BalancedBaggingClassifier(base_estimator = RandomForestClassifier(criterion='entropy'), n_estimators = 10, bootstrap = True)\n",
    "    y = bb_classifier.fit(data_feature,data_label)\n",
    "    predicted_data = y.predict(data_feature1)\n",
    "    #predicted_data = pd.DataFrame(predicted_data)\n",
    "    np.savetxt(\"foo.csv\",predicted_data, delimiter=\",\")\n",
    "    aa = accuracy_score(predicted_data,data_label1)\n",
    "    bb = confusion_matrix(predicted_data,data_label1)\n",
    "    return aa, bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in b:\\downloads\\anaconda\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in b:\\downloads\\anaconda\\lib\\site-packages (from imblearn) (0.3.3)\n",
      "Requirement already satisfied: numpy in b:\\downloads\\anaconda\\lib\\site-packages (from imbalanced-learn->imblearn) (1.14.3)\n",
      "Requirement already satisfied: scikit-learn in b:\\downloads\\anaconda\\lib\\site-packages (from imbalanced-learn->imblearn) (0.19.1)\n",
      "Requirement already satisfied: scipy in b:\\downloads\\anaconda\\lib\\site-packages (from imbalanced-learn->imblearn) (1.1.0)\n",
      "Original Length= 7027 \tCleaned Length= 3194 \tMissing Data= 3833\n",
      "Original Length= 10173 \tCleaned Length= 4088 \tMissing Data= 6085\n",
      "Dataset: 1styear\n",
      "Y\n",
      "0    6756\n",
      "1     271\n",
      "dtype: int64\n",
      "Minority (label 1) percentage: 3.856553294435748%\n",
      "----------------------------------------------------------------\n",
      "Dataset: 1styear\n",
      "Y\n",
      "0    9773\n",
      "1     400\n",
      "dtype: int64\n",
      "Minority (label 1) percentage: 3.931976801336872%\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7714110303898496, array([[8999, 3694],\n",
       "        [ 774, 6079]], dtype=int64))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_read('1year.arff','2year.arff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
