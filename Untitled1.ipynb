{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in c:\\users\\akashdudi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\akashdudi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from imblearn) (0.3.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\akashdudi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (0.19.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\akashdudi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\akashdudi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.14.1)\n"
     ]
    }
   ],
   "source": [
    "# To supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "# For loading .arff files\n",
    "from scipy.io import arff\n",
    "\n",
    "# To perform mean imputation\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Formatted counter of class labels\n",
    "from collections import Counter\n",
    "# Ordered Dictionary\n",
    "from collections import OrderedDict\n",
    "#To perform kFold Cross Validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Library imbalanced-learn to deal with the data imbalance. To use SMOTE oversampling\n",
    "!pip install imblearn\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "# Impoting classification models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X56</th>\n",
       "      <th>X57</th>\n",
       "      <th>X58</th>\n",
       "      <th>X59</th>\n",
       "      <th>X60</th>\n",
       "      <th>X61</th>\n",
       "      <th>X62</th>\n",
       "      <th>X63</th>\n",
       "      <th>X64</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.200550</td>\n",
       "      <td>0.37951</td>\n",
       "      <td>0.39641</td>\n",
       "      <td>2.0472</td>\n",
       "      <td>32.3510</td>\n",
       "      <td>0.38825</td>\n",
       "      <td>0.249760</td>\n",
       "      <td>1.33050</td>\n",
       "      <td>1.1389</td>\n",
       "      <td>0.50494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121960</td>\n",
       "      <td>0.39718</td>\n",
       "      <td>0.87804</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>8.4160</td>\n",
       "      <td>5.1372</td>\n",
       "      <td>82.658</td>\n",
       "      <td>4.4158</td>\n",
       "      <td>7.4277</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.209120</td>\n",
       "      <td>0.49988</td>\n",
       "      <td>0.47225</td>\n",
       "      <td>1.9447</td>\n",
       "      <td>14.7860</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.258340</td>\n",
       "      <td>0.99601</td>\n",
       "      <td>1.6996</td>\n",
       "      <td>0.49788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>0.42002</td>\n",
       "      <td>0.85300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.1486</td>\n",
       "      <td>3.2732</td>\n",
       "      <td>107.350</td>\n",
       "      <td>3.4000</td>\n",
       "      <td>60.9870</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.248660</td>\n",
       "      <td>0.69592</td>\n",
       "      <td>0.26713</td>\n",
       "      <td>1.5548</td>\n",
       "      <td>-1.1523</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.309060</td>\n",
       "      <td>0.43695</td>\n",
       "      <td>1.3090</td>\n",
       "      <td>0.30408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241140</td>\n",
       "      <td>0.81774</td>\n",
       "      <td>0.76599</td>\n",
       "      <td>0.694840</td>\n",
       "      <td>4.9909</td>\n",
       "      <td>3.9510</td>\n",
       "      <td>134.270</td>\n",
       "      <td>2.7185</td>\n",
       "      <td>5.2078</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.081483</td>\n",
       "      <td>0.30734</td>\n",
       "      <td>0.45879</td>\n",
       "      <td>2.4928</td>\n",
       "      <td>51.9520</td>\n",
       "      <td>0.14988</td>\n",
       "      <td>0.092704</td>\n",
       "      <td>1.86610</td>\n",
       "      <td>1.0571</td>\n",
       "      <td>0.57353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054015</td>\n",
       "      <td>0.14207</td>\n",
       "      <td>0.94598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.5746</td>\n",
       "      <td>3.6147</td>\n",
       "      <td>86.435</td>\n",
       "      <td>4.2228</td>\n",
       "      <td>5.5497</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.187320</td>\n",
       "      <td>0.61323</td>\n",
       "      <td>0.22960</td>\n",
       "      <td>1.4063</td>\n",
       "      <td>-7.3128</td>\n",
       "      <td>0.18732</td>\n",
       "      <td>0.187320</td>\n",
       "      <td>0.63070</td>\n",
       "      <td>1.1559</td>\n",
       "      <td>0.38677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134850</td>\n",
       "      <td>0.48431</td>\n",
       "      <td>0.86515</td>\n",
       "      <td>0.124440</td>\n",
       "      <td>6.3985</td>\n",
       "      <td>4.3158</td>\n",
       "      <td>127.210</td>\n",
       "      <td>2.8692</td>\n",
       "      <td>7.8980</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1       X2       X3      X4       X5       X6        X7       X8  \\\n",
       "0  0.200550  0.37951  0.39641  2.0472  32.3510  0.38825  0.249760  1.33050   \n",
       "1  0.209120  0.49988  0.47225  1.9447  14.7860  0.00000  0.258340  0.99601   \n",
       "2  0.248660  0.69592  0.26713  1.5548  -1.1523  0.00000  0.309060  0.43695   \n",
       "3  0.081483  0.30734  0.45879  2.4928  51.9520  0.14988  0.092704  1.86610   \n",
       "4  0.187320  0.61323  0.22960  1.4063  -7.3128  0.18732  0.187320  0.63070   \n",
       "\n",
       "       X9      X10  ...        X56      X57      X58       X59     X60  \\\n",
       "0  1.1389  0.50494  ...   0.121960  0.39718  0.87804  0.001924  8.4160   \n",
       "1  1.6996  0.49788  ...   0.121300  0.42002  0.85300  0.000000  4.1486   \n",
       "2  1.3090  0.30408  ...   0.241140  0.81774  0.76599  0.694840  4.9909   \n",
       "3  1.0571  0.57353  ...   0.054015  0.14207  0.94598  0.000000  4.5746   \n",
       "4  1.1559  0.38677  ...   0.134850  0.48431  0.86515  0.124440  6.3985   \n",
       "\n",
       "      X61      X62     X63      X64     Y  \n",
       "0  5.1372   82.658  4.4158   7.4277  b'0'  \n",
       "1  3.2732  107.350  3.4000  60.9870  b'0'  \n",
       "2  3.9510  134.270  2.7185   5.2078  b'0'  \n",
       "3  3.6147   86.435  4.2228   5.5497  b'0'  \n",
       "4  4.3158  127.210  2.8692   7.8980  b'0'  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads the 5 raw .arff files into a list\n",
    "def load_arff_raw_data():\n",
    "    N=5\n",
    "    return [arff.loadarff('data/' + str(i+1) + 'year.arff') for i in range(N)]\n",
    "\n",
    "# Loads the 5 raw .arff files into pandas dataframes\n",
    "def load_dataframes():\n",
    "    return [pd.DataFrame(data_i_year[0]) for data_i_year in load_arff_raw_data()]\n",
    "\n",
    "# Set the column headers from X1 ... X64 and the class label as Y, for all the 5 dataframes.\n",
    "def set_new_headers(dataframes):\n",
    "    cols = ['X' + str(i+1) for i in range(len(dataframes[0].columns)-1)]\n",
    "    cols.append('Y')\n",
    "    for df in dataframes:\n",
    "        df.columns = cols\n",
    "\n",
    "# dataframes is the list of pandas dataframes for the 5 year datafiles.  \n",
    "dataframes = load_dataframes()\n",
    "\n",
    "# Set the new headers for the dataframes. The new headers will have the renamed set of feature (X1 to X64)\n",
    "set_new_headers(dataframes)  \n",
    "\n",
    "# print the first 5 rows of a dataset 'year1'\n",
    "dataframes[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dtypes of all the columns (other than the class label columns) to float.\n",
    "def convert_columns_type_float(dfs):\n",
    "    for i in range(5):\n",
    "        index = 1\n",
    "        while(index<=63):\n",
    "            colname = dfs[i].columns[index]\n",
    "            col = getattr(dfs[i], colname)\n",
    "            dfs[i][colname] = col.astype(float)\n",
    "            index+=1\n",
    "            \n",
    "convert_columns_type_float(dataframes)  \n",
    "\n",
    "# The class labels for all the dataframes are originally in object type.\n",
    "# Convert them to int types\n",
    "def convert_class_label_type_int(dfs):\n",
    "    for i in range(len(dfs)):\n",
    "        col = getattr(dfs[i], 'Y')\n",
    "        dfs[i]['Y'] = col.astype(int)\n",
    "        \n",
    "convert_class_label_type_int(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1year: Original Length= 7027 \tCleaned Length= 3194 \tMissing Data= 3833\n",
      "2year: Original Length= 10173 \tCleaned Length= 4088 \tMissing Data= 6085\n",
      "3year: Original Length= 10503 \tCleaned Length= 4885 \tMissing Data= 5618\n",
      "4year: Original Length= 9792 \tCleaned Length= 4769 \tMissing Data= 5023\n",
      "5year: Original Length= 5910 \tCleaned Length= 3031 \tMissing Data= 2879\n"
     ]
    }
   ],
   "source": [
    "# Get Clean dataframes by dropping all the rows which have missing values\n",
    "def drop_nan_rows(dataframes, verbose=False):\n",
    "    clean_dataframes = [df.dropna(axis=0, how='any') for df in dataframes]\n",
    "    if verbose:\n",
    "        for i in range(len(dataframes)):\n",
    "            print(str(i+1)+'year:','Original Length=', len(dataframes[i]), '\\tCleaned Length=', len(clean_dataframes[i]), '\\tMissing Data=', len(dataframes[i])-len(clean_dataframes[i]))\n",
    "    return clean_dataframes\n",
    "\n",
    "# Doing a quick analysis of how many missing values are there in each of the 5 dataframes\n",
    "nan_dropped_dataframes = drop_nan_rows(dataframes, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_mean_imputation(dfs):\n",
    "    # Construct an imputer with strategy as 'mean', to mean-impute along the columns\n",
    "    imputer = Imputer(missing_values=np.nan, strategy='mean', axis=0)\n",
    "    mean_imputed_dfs = [pd.DataFrame(imputer.fit_transform(df)) for df in dfs]\n",
    "    for i in range(len(dfs)):\n",
    "        mean_imputed_dfs[i].columns = dfs[i].columns   \n",
    "    return mean_imputed_dfs\n",
    "\n",
    "mean_imputed_dataframes = perform_mean_imputation(dataframes)\n",
    "\n",
    "imputed_dataframes_dictionary = OrderedDict()\n",
    "imputed_dataframes_dictionary['Mean'] = mean_imputed_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1year\n",
      "Y\n",
      "0    6756\n",
      "1     271\n",
      "dtype: int64\n",
      "Minority (label 1) percentage: 3.856553294435748%\n",
      "----------------------------------------------------------------\n",
      "Dataset: 2year\n",
      "Y\n",
      "0    9773\n",
      "1     400\n",
      "dtype: int64\n",
      "Minority (label 1) percentage: 3.931976801336872%\n",
      "----------------------------------------------------------------\n",
      "Dataset: 3year\n",
      "Y\n",
      "0    10008\n",
      "1      495\n",
      "dtype: int64\n",
      "Minority (label 1) percentage: 4.712939160239932%\n",
      "----------------------------------------------------------------\n",
      "Dataset: 4year\n",
      "Y\n",
      "0    9277\n",
      "1     515\n",
      "dtype: int64\n",
      "Minority (label 1) percentage: 5.259395424836601%\n",
      "----------------------------------------------------------------\n",
      "Dataset: 5year\n",
      "Y\n",
      "0    5500\n",
      "1     410\n",
      "dtype: int64\n",
      "Minority (label 1) percentage: 6.937394247038917%\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def check_data_imbalance(dfs):\n",
    "    for i in range(len(dfs)):\n",
    "        print('Dataset: '+str(i+1)+'year')\n",
    "        print(dfs[i].groupby('Y').size())\n",
    "        minority_percent = (dfs[i]['Y'].tolist().count(1) / len(dfs[i]['Y'].tolist()))*100\n",
    "        print('Minority (label 1) percentage: '+  str(minority_percent) + '%')\n",
    "        print('-'*64)\n",
    "        \n",
    "check_data_imbalance(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the features and labels into separate dataframes for all the original dataframes\n",
    "def split_dataframes_features_labels(dfs):\n",
    "    feature_dfs = [dfs[i].iloc[:,0:64] for i in range(len(dfs))]\n",
    "    label_dfs = [dfs[i].iloc[:,64] for i in range(len(dfs))]\n",
    "    return feature_dfs, label_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE Oversampling for Mean imputed dataframes\n",
      "\n",
      "Dataset: 1year:\n",
      "Original dataset shape Counter({0.0: 6756, 1.0: 271})\n",
      "Resampled dataset shape Counter({0.0: 6756, 1.0: 6756})\n",
      "\n",
      "Dataset: 2year:\n",
      "Original dataset shape Counter({0.0: 9773, 1.0: 400})\n",
      "Resampled dataset shape Counter({0.0: 9773, 1.0: 9773})\n",
      "\n",
      "Dataset: 3year:\n",
      "Original dataset shape Counter({0.0: 10008, 1.0: 495})\n",
      "Resampled dataset shape Counter({0.0: 10008, 1.0: 10008})\n",
      "\n",
      "Dataset: 4year:\n",
      "Original dataset shape Counter({0.0: 9277, 1.0: 515})\n",
      "Resampled dataset shape Counter({0.0: 9277, 1.0: 9277})\n",
      "\n",
      "Dataset: 5year:\n",
      "Original dataset shape Counter({0.0: 5500, 1.0: 410})\n",
      "Resampled dataset shape Counter({0.0: 5500, 1.0: 5500})\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Performs the SMOTE oversampling fro given dataframes.\n",
    "def oversample_data_SMOTE(dfs, verbose=False):\n",
    "    smote = SMOTE(ratio='auto' , random_state=42, k_neighbors=10)\n",
    "    #Split the features and labels for each dataframe\n",
    "    feature_dfs, label_dfs = split_dataframes_features_labels(dfs)\n",
    "    resampled_feature_arrays = []\n",
    "    resampled_label_arrays = []\n",
    "    for i in range(len(dfs)):\n",
    "        if verbose: print('Dataset: ' + str(i+1) + 'year:')\n",
    "        if verbose: print('Original dataset shape {}'.format(Counter(label_dfs[i])))\n",
    "        dfi_features_res, dfi_label_res = smote.fit_sample(feature_dfs[i], label_dfs[i])\n",
    "        if verbose: print('Resampled dataset shape {}\\n'.format(Counter(dfi_label_res)))\n",
    "        # Append the resampled feature and label arrays of ith dataframe to their respective list of arrays    \n",
    "        resampled_feature_arrays.append(dfi_features_res)\n",
    "        resampled_label_arrays.append(dfi_label_res)        \n",
    "    return resampled_feature_arrays, resampled_label_arrays\n",
    "\n",
    "\n",
    "# Utility Function to convert the arrays of features and labels to pandas dataframes, and then join them.\n",
    "# Also re-assign the columns headers.\n",
    "def restructure_arrays_to_dataframes(feature_arrays, label_arrays):\n",
    "    resampled_dfs = []\n",
    "    for i in range(len(feature_arrays)):\n",
    "        feature_df = pd.DataFrame(data=feature_arrays[i])\n",
    "        label_df = pd.DataFrame(data=label_arrays[i])\n",
    "        # Must set the column header for label_df, otherwise it wont join with feature_df, as columns overlap (with col names '0')\n",
    "        label_df.columns=['Y'] \n",
    "        resampled_dfs.append(feature_df.join(label_df))\n",
    "    # re-assign the column headers for features and labels    \n",
    "    set_new_headers(resampled_dfs)    \n",
    "    return resampled_dfs\n",
    "\n",
    "# Perform SMOTE oversampling on all the imputed dataframes, and return them in a dictionary.\n",
    "def perform_oversampling_on_imputed_dataframes(df_dict):\n",
    "    imputed_oversampled_dataframes_dictionary = OrderedDict()\n",
    "    for key,dfs in df_dict.items():\n",
    "        print('SMOTE Oversampling for ' + key + ' imputed dataframes\\n')\n",
    "        smote_feature_arrays, smote_label_arrays = oversample_data_SMOTE(dfs, verbose=True)\n",
    "        oversampled_dataframes = restructure_arrays_to_dataframes(smote_feature_arrays, smote_label_arrays)\n",
    "        imputed_oversampled_dataframes_dictionary[key] = oversampled_dataframes\n",
    "        print('-'*100)\n",
    "    return imputed_oversampled_dataframes_dictionary\n",
    "\n",
    "imputed_oversampled_dataframes_dictionary = perform_oversampling_on_imputed_dataframes(imputed_dataframes_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_kfold_cv_data(k, X, y, verbose=False):\n",
    "    X = X.values\n",
    "    y = y.values\n",
    "    kf = KFold(n_splits=k, shuffle=False, random_state=42)\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train.append(X[train_index])\n",
    "        y_train.append(y[train_index])\n",
    "        X_test.append(X[test_index])\n",
    "        y_test.append(y[test_index])\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced Bagging Classifier\n",
    "bb_classifier = BalancedBaggingClassifier(base_estimator = RandomForestClassifier(criterion='entropy'), n_estimators = 10, bootstrap = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary of models\n",
    "models_dictionary = OrderedDict()\n",
    "\n",
    "models_dictionary['Balanced Bagging'] = bb_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform data modeling\n",
    "def perform_data_modeling(_models_, _imputers_, verbose=False, k_folds=5):\n",
    "    \n",
    "    # 7 Models\n",
    "    # 4 Imputers\n",
    "    # 5 datasets (for 5 years)\n",
    "    # 7 metrics, averaged over all the K-Folds\n",
    "    model_results = OrderedDict()\n",
    "    \n",
    "    # Iterate over the models\n",
    "    for model_name, clf in _models_.items():\n",
    "        if verbose: print(\"-\"*120, \"\\n\", \"Model: \" + '\\033[1m' + model_name + '\\033[0m' + \" Classifier\")\n",
    "        imputer_results = OrderedDict()\n",
    "        \n",
    "        # Iterate over the different imputed_data mechanisms (Mean, k-NN, EM, MICE)\n",
    "        for imputer_name, dataframes_list in _imputers_.items():\n",
    "            if verbose: print('\\tImputer Technique: ' + '\\033[1m' + imputer_name + '\\033[0m')\n",
    "            \n",
    "            # call the split_dataframes_features_labels function to get a list of features and labels for all the dataframes\n",
    "            feature_dfs, label_dfs = split_dataframes_features_labels(dataframes_list)            \n",
    "            \n",
    "            year_results = OrderedDict()\n",
    "            \n",
    "            # Iterate over dataframe_list individually\n",
    "            for df_index in range(len(dataframes_list)):\n",
    "                if verbose: print('\\t\\tDataset: ' + '\\033[1m' + str(df_index+1) + 'year' + '\\033[0m')\n",
    "                \n",
    "                # Calling the 'prepare_kfold_cv_data' returns lists of features and labels \n",
    "                # for train and test sets respectively.\n",
    "                # The number of items in the list is equal to k_folds\n",
    "                X_train_list, y_train_list, X_test_list, y_test_list = prepare_kfold_cv_data(k_folds, feature_dfs[df_index], label_dfs[df_index], verbose)\n",
    "                \n",
    "                metrics_results = OrderedDict()\n",
    "                accuracy_list = np.zeros([k_folds])\n",
    "                precision_list = np.zeros([k_folds,2])\n",
    "                recall_list = np.zeros([k_folds,2])\n",
    "                TN_list = np.zeros([k_folds])\n",
    "                FP_list = np.zeros([k_folds])\n",
    "                FN_list = np.zeros([k_folds])\n",
    "                TP_list = np.zeros([k_folds])                \n",
    "                \n",
    "                # Iterate over all the k-folds\n",
    "                for k_index in range(k_folds):\n",
    "                    X_train = X_train_list[k_index]\n",
    "                    y_train = y_train_list[k_index]\n",
    "                    X_test = X_test_list[k_index]\n",
    "                    y_test = y_test_list[k_index]\n",
    "                    \n",
    "                    # Fit the model and \n",
    "                    clf = clf.fit(X_train, y_train)\n",
    "                    y_test_predicted = clf.predict(X_test)\n",
    "                    \n",
    "                    #code for calculating accuracy \n",
    "                    _accuracy_ = accuracy_score(y_test, y_test_predicted, normalize=True)\n",
    "                    accuracy_list[k_index] = _accuracy_\n",
    "                    \n",
    "                    #code for calculating recall \n",
    "                    _recalls_ = recall_score(y_test, y_test_predicted, average=None)\n",
    "                    recall_list[k_index] = _recalls_\n",
    "                    \n",
    "                    #code for calculating precision \n",
    "                    _precisions_ = precision_score(y_test, y_test_predicted, average=None)\n",
    "                    precision_list[k_index] = _precisions_\n",
    "                    \n",
    "                    #code for calculating confusion matrix \n",
    "                    _confusion_matrix_ = confusion_matrix(y_test, y_test_predicted)\n",
    "                    TN_list[k_index] = _confusion_matrix_[0][0]\n",
    "                    FP_list[k_index] = _confusion_matrix_[0][1]\n",
    "                    FN_list[k_index] = _confusion_matrix_[1][0]\n",
    "                    TP_list[k_index] = _confusion_matrix_[1][1]\n",
    "                \n",
    "                # creating a metrics dictionary\n",
    "                metrics_results['Accuracy'] = np.mean(accuracy_list)\n",
    "                metrics_results['Precisions'] = np.mean(precision_list, axis=0)\n",
    "                metrics_results['Recalls'] = np.mean(recall_list, axis=0)\n",
    "                metrics_results['TN'] = np.mean(TN_list)\n",
    "                metrics_results['FP'] = np.mean(FP_list)\n",
    "                metrics_results['FN'] = np.mean(FN_list)\n",
    "                metrics_results['TP'] = np.mean(TP_list)\n",
    "                \n",
    "                if verbose:\n",
    "                    print('\\t\\t\\tAccuracy:', metrics_results['Accuracy'])\n",
    "                    print('\\t\\t\\tPrecision:', metrics_results['Precisions'])\n",
    "                    print('\\t\\t\\tRecall:', metrics_results['Recalls'])\n",
    "                \n",
    "                year_results[str(df_index+1)+'year'] = metrics_results   \n",
    "                \n",
    "            imputer_results[imputer_name] = year_results\n",
    "            \n",
    "        model_results[model_name] = imputer_results  \n",
    "        \n",
    "    return model_results                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------ \n",
      " Model: \u001b[1mBalanced Bagging\u001b[0m Classifier\n",
      "\tImputer Technique: \u001b[1mMean\u001b[0m\n",
      "\t\tDataset: \u001b[1m1year\u001b[0m\n",
      "\t\t\tAccuracy: 0.9796491164654345\n",
      "\t\t\tPrecision: [0.59717892 0.59675277]\n",
      "\t\t\tRecall: [0.58482805 0.59178594]\n",
      "\t\tDataset: \u001b[1m2year\u001b[0m\n",
      "\t\t\tAccuracy: 0.9753407540733268\n",
      "\t\t\tPrecision: [0.59532758 0.59680412]\n",
      "\t\t\tRecall: [0.58291221 0.58848931]\n",
      "\t\tDataset: \u001b[1m3year\u001b[0m\n",
      "\t\t\tAccuracy: 0.9630300501397178\n",
      "\t\t\tPrecision: [0.59359359 0.59331671]\n",
      "\t\t\tRecall: [0.57167189 0.58481299]\n",
      "\t\tDataset: \u001b[1m4year\u001b[0m\n",
      "\t\t\tAccuracy: 0.9586621982726374\n",
      "\t\t\tPrecision: [0.59443535 0.59222577]\n",
      "\t\t\tRecall: [0.56523636 0.58674236]\n",
      "\t\tDataset: \u001b[1m5year\u001b[0m\n",
      "\t\t\tAccuracy: 0.9532727272727273\n",
      "\t\t\tPrecision: [0.59329609 0.58898757]\n",
      "\t\t\tRecall: [0.56336364 0.581     ]\n"
     ]
    }
   ],
   "source": [
    "results = perform_data_modeling(models_dictionary, imputed_oversampled_dataframes_dictionary, verbose=True, k_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This list stores results of Balanced Bagging classifier obtained by running it for \n",
    "# various values of number of estimators in the range of 1 to 30\n",
    "\n",
    "#results_by_estimators = []\n",
    "#for i in range(29):\n",
    "#   models_dictionary['Balanced Bagging'] = BalancedBaggingClassifier(base_estimator = RandomForestClassifier(criterion='entropy'), n_estimators = 1+i, bootstrap = True)\n",
    "#   results = perform_data_modeling(models_dictionary, imputed_oversampled_dataframes_dictionary, verbose=True, k_folds=5)\n",
    "#   results_by_estimators.append(results) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "year1_values = []\n",
    "year2_values = []\n",
    "year3_values = []\n",
    "year4_values = []\n",
    "year5_values = []\n",
    "\n",
    "# extract corresponding Balanced bagging with Mean imputation\n",
    "# classification metrics \n",
    "#def extract_actual_values_from_dict(curr_dict):\n",
    "#    temp_dict = curr_dict['Balanced Bagging']\n",
    "#    return temp_dict['Mean']\n",
    "temp_dict = results['Balanced Bagging']\n",
    "curr_result = temp_dict['Mean']\n",
    "\n",
    "#for i in range(29):\n",
    "#    curr_dict = results_by_estimators[i]\n",
    "#    curr_result = extract_actual_values_from_dict(curr_dict)\n",
    "    \n",
    "        \n",
    "year_1_result = curr_result['1year']\n",
    "year_2_result = curr_result['2year']\n",
    "year_3_result = curr_result['3year']\n",
    "year_4_result = curr_result['4year']\n",
    "year_5_result = curr_result['5year']\n",
    "year1_values.append(year_1_result['Accuracy'])\n",
    "year2_values.append(year_2_result['Accuracy'])\n",
    "year3_values.append(year_3_result['Accuracy'])\n",
    "year4_values.append(year_4_result['Accuracy'])\n",
    "year5_values.append(year_5_result['Accuracy'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "estimators = [i+1 for i in range(29)] \n",
    "\n",
    "# plot year1, year2, year3, year4 and year5 accuracy values\n",
    "# for range of estimator values from 1 to 30\n",
    "plt.plot(estimators, year1_values, '.b-')\n",
    "plt.plot(estimators, year2_values, '.r-')\n",
    "plt.plot(estimators, year3_values, '.y-')\n",
    "plt.plot(estimators, year4_values, '.g-')\n",
    "plt.plot(estimators, year5_values, '.m-') \n",
    "plt.xlabel(\"\\nNumber of estimators\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"\\nEffect of varying number of estimators on the accuracy scores on different datasets\\n\")\n",
    "\n",
    "# display legend\n",
    "plt.plot(10, 0.93, '.b-', label='Year 1')\n",
    "plt.plot(10, 0.93, '.r-', label='Year 2')\n",
    "plt.plot(10, 0.93, '.y-', label='Year 3')\n",
    "plt.plot(10, 0.93, '.g-', label='Year 4')\n",
    "plt.plot(10, 0.93, '.m-', label='Year 5')\n",
    "\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.982165469570368]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year1_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Accuracy', 0.982165469570368),\n",
       "             ('Precisions', array([0.59776119, 0.59632893])),\n",
       "             ('Recalls', array([0.58519752, 0.59400608])),\n",
       "             ('TN', 1316.2),\n",
       "             ('FP', 35.0),\n",
       "             ('FN', 13.2),\n",
       "             ('TP', 1338.0)])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_1_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
