{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_read(file,model):\n",
    "    # To supress warnings\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    #!pip install msgpack-python\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    #!pip install pickle  --upgrade pip\n",
    "    import pickle\n",
    "    %matplotlib inline\n",
    "\n",
    "    # For loading .arff files\n",
    "    from scipy.io import arff\n",
    "\n",
    "    # To perform mean imputation\n",
    "    from sklearn.preprocessing import Imputer\n",
    "\n",
    "    # Formatted counter of class labels\n",
    "    from collections import Counter\n",
    "    # Ordered Dictionary\n",
    "    from collections import OrderedDict\n",
    "    #To perform kFold Cross Validation\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Library imbalanced-learn to deal with the data imbalance. To use SMOTE oversampling\n",
    "    from imblearn.over_sampling import SMOTE \n",
    "\n",
    "    # Impoting classification models\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from imblearn.ensemble import BalancedBaggingClassifier\n",
    "\n",
    "    import random\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    \n",
    "    \n",
    "    df1, meta1 = arff.loadarff(file);\n",
    "    df1 = pd.DataFrame(df1);\n",
    "    \n",
    "    \n",
    "    def set_new_headers(dataframes):\n",
    "        cols = ['X' + str(i+1) for i in range(len(dataframes.columns)-1)]\n",
    "        cols.append('Y')\n",
    "        dataframes.columns = cols\n",
    "    set_new_headers(df1)\n",
    "    \n",
    "    \n",
    "    # Convert the dtypes of all the columns (other than the class label columns) to float.\n",
    "    def convert_columns_type_float(dfs):\n",
    "        index = 1\n",
    "        while(index<=63):\n",
    "            colname = dfs.columns[index]\n",
    "            col = getattr(dfs, colname)\n",
    "            dfs[colname] = col.astype(float)\n",
    "            index+=1\n",
    "\n",
    "    convert_columns_type_float(df1)\n",
    "\n",
    "\n",
    "    # The class labels for all the dataframes are originally in object type.\n",
    "    # Convert them to int types\n",
    "    def convert_class_label_type_int(dfs):\n",
    "        col = getattr(dfs, 'Y')\n",
    "        dfs['Y'] = col.astype(int)\n",
    "\n",
    "        \n",
    "    convert_class_label_type_int(df1)\n",
    "    \n",
    "    # Get Clean dataframes by dropping all the rows which have missing values\n",
    "    def drop_nan_rows(dataframes, verbose=False):\n",
    "        clean_dataframes = dataframes.dropna(axis=0, how='any')\n",
    "        if verbose:\n",
    "            print('Original Length=', len(dataframes), '\\tCleaned Length=', len(clean_dataframes), \n",
    "                  '\\tMissing Data=', len(dataframes)-len(clean_dataframes))\n",
    "        return clean_dataframes\n",
    "\n",
    "    # Doing a quick analysis of how many missing values are there in each of the 5 dataframes\n",
    "    nan_dropped_dataframes = drop_nan_rows(df1, verbose=True)\n",
    "    \n",
    "    def perform_mean_imputation(dfs):\n",
    "        # Construct an imputer with strategy as 'mean', to mean-impute along the columns\n",
    "        imputer = Imputer(missing_values=np.nan, strategy='mean', axis=0)\n",
    "        mean_imputed_dfs = pd.DataFrame(imputer.fit_transform(dfs))\n",
    "        mean_imputed_dfs.columns = dfs.columns   \n",
    "        return mean_imputed_dfs\n",
    "\n",
    "    mean_imputed_dataframes = perform_mean_imputation(df1)\n",
    "    \n",
    "    \n",
    "    def check_data_imbalance(dfs):\n",
    "        print('Dataset: '+file)\n",
    "        print(dfs.groupby('Y').size())\n",
    "        minority_percent = (dfs['Y'].tolist().count(1) / len(dfs['Y'].tolist()))*100\n",
    "        print('Minority (label 1) percentage: '+  str(minority_percent) + '%')\n",
    "        print('-'*64)\n",
    "\n",
    "    check_data_imbalance(df1)\n",
    "    \n",
    "    # Split the features and labels into separate dataframes for all the original dataframes\n",
    "    def split_dataframes_features_labels(dfs):\n",
    "        feature_dfs = dfs.iloc[:,0:64] \n",
    "        label_dfs = dfs.iloc[:,64]\n",
    "        return feature_dfs, label_dfs\n",
    "    \n",
    "    feature,label = split_dataframes_features_labels(mean_imputed_dataframes)\n",
    "    \n",
    "    m1 = SMOTE()\n",
    "    x,y = m1.fit_sample(feature,label)\n",
    "    x = pd.DataFrame(x)\n",
    "    y = pd.DataFrame(y)\n",
    "    data = pd.concat([x,y],axis = 1)\n",
    "    set_new_headers(data)\n",
    "    data_feature = data.iloc[:,0:64] \n",
    "    data_label = data.iloc[:,64:]\n",
    "    \n",
    "\n",
    "    # Balanced Bagging Classifier\n",
    "    loaded_model = pickle.load(open(model, 'rb'))\n",
    "    predicted_data = loaded_model.predict(data_feature)\n",
    "    \n",
    "    #predicted_data = pd.DataFrame(predicted_data)\n",
    "    np.savetxt(\"returnn.csv\",predicted_data, delimiter=\",\")\n",
    "    aa = accuracy_score(predicted_data,data_label)\n",
    "    bb = confusion_matrix(predicted_data,data_label)\n",
    "    return aa, bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Length= 10503 \tCleaned Length= 4885 \tMissing Data= 5618\n",
      "Dataset: 3year.arff\n",
      "Y\n",
      "0    10008\n",
      "1      495\n",
      "dtype: int64\n",
      "Minority (label 1) percentage: 4.712939160239932%\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7743804956035172, array([[9757, 4265],\n",
       "        [ 251, 5743]], dtype=int64))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_read('3year.arff','final_model.sav')          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
